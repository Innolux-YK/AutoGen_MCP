"""
LangChain æ™ºèƒ½ä»£ç† - ä½¿ç”¨ LLM è‡ªå‹•è¦åŠƒå’ŒåŸ·è¡Œä»»å‹™
"""

from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from typing import Dict, Any, List
import sys
import os

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config
from tools.tool_manager import ToolManager

class LangChainAgent:
    """åŸºæ–¼ LangChain çš„æ™ºèƒ½ä»£ç†"""
    
    def __init__(self):
        self.tool_manager = ToolManager()
        self.llm = ChatOpenAI(
            api_key=config.API_KEY,
            base_url=config.get_api_url(),
            model=getattr(config, "INNOAI_DEFAULT_MODEL", "gpt-4"),
            temperature=0.1,
            max_tokens=4000  # å¢åŠ æœ€å¤§è¼¸å‡ºé•·åº¦
        )
        self.tools = self.tool_manager.get_langchain_tools()
        self.agent_executor = self._create_agent()
        print("ğŸ¤– LangChain æ™ºèƒ½ä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    def _create_tools(self) -> List:
        """å‰µå»ºå·¥å…·åˆ—è¡¨ (å·²ç”± ToolManager è™•ç†)"""
        return self.tool_manager.get_langchain_tools()
    
    def _create_agent(self) -> AgentExecutor:
        """å‰µå»º LangChain Agent"""
        
        # è‡ªå®šç¾© prompt æ¨¡æ¿ - ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¸¦åŠ å¼·æ ¼å¼ç´„æŸ
        template = """You are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:

{tools}

IMPORTANT: You MUST follow this exact format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

CRITICAL FORMATTING RULES:
1. Always include "Action:" before specifying the action
2. Always include "Action Input:" before providing the input
3. Always include "Thought:" before your reasoning
4. Always include "Final Answer:" before your conclusion
5. Never skip any of these formatting elements

Begin!

Question: {input}
Thought:{agent_scratchpad}"""

        prompt = PromptTemplate.from_template(template)
        
        # å‰µå»º ReAct agent
        agent = create_react_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
        
        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=5,
            handle_parsing_errors=True,
            return_intermediate_steps=True,
            max_execution_time=None
            # ç§»é™¤ early_stopping_method åƒæ•¸ï¼Œé¿å…æ¨¡å‹å…¼å®¹æ€§å•é¡Œ
        )
    
    def solve_problem(self, query: str, llm_model: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM è‡ªå‹•è¦åŠƒä¸¦è§£æ±ºå•é¡Œ
        
        Args:
            query: ç”¨æˆ¶å•é¡Œ
            llm_model: æŒ‡å®šä½¿ç”¨çš„LLMæ¨¡å‹
            
        Returns:
            è§£æ±ºæ–¹æ¡ˆå’ŒåŸ·è¡Œéç¨‹
        """
        try:
            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œè‡¨æ™‚æ›´æ–°LLM
            original_model = self.llm.model_name
            original_llm = self.llm  # ä¿å­˜åŸå§‹LLMå¼•ç”¨
            
            if llm_model and llm_model != original_model:
                print(f"ğŸ”„ åˆ‡æ›æ¨¡å‹: {original_model} â†’ {llm_model}")
                # å‰µå»ºæ–°çš„LLMå¯¦ä¾‹è€Œä¸æ˜¯ä¿®æ”¹ç¾æœ‰çš„
                temp_llm = ChatOpenAI(
                    api_key=config.API_KEY,
                    base_url=config.get_api_url(llm_model),
                    model=llm_model,
                    temperature=0.1,
                    max_tokens=4000
                )
                # æš«æ™‚æ›¿æ›
                self.llm = temp_llm
                # é‡æ–°å‰µå»ºagent executor
                self.agent_executor = self._create_agent()
            
            print(f"ğŸ¤– LangChain Agent è™•ç†å•é¡Œ (æ¨¡å‹: {self.llm.model_name}): {query}")
            
            # ç‰¹æ®Šè™•ç† SPC æŸ¥è©¢ - ç›´æ¥èª¿ç”¨å·¥å…·é¿å…æˆªæ–·
            if "SPC" in query and ("é€²CHART" in query or "æ²’æœ‰é€²" in query or "CHART" in query):
                print("ğŸ” æª¢æ¸¬åˆ° SPC æŸ¥è©¢ï¼Œç›´æ¥ä½¿ç”¨å·¥å…·é¿å…è¼¸å‡ºæˆªæ–·")
                spc_result = self.tool_manager.execute_tool("spc_query", query)
                current_model = llm_model or self.llm.model_name
                model_prefix = f"**{current_model.upper()}**: "
                return {
                    "answer": model_prefix + spc_result,
                    "confidence": 0.9,
                    "source": "direct_spc_tool",
                    "execution_steps": [{"tool": "spc_query", "input": query, "output": spc_result}],
                    "total_steps": 1,
                    "model_used": current_model
                }
            
            # åŸ·è¡Œ agent
            result = self.agent_executor.invoke({"input": query})
            
            # æå–åŸ·è¡Œæ­¥é©Ÿ
            steps = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    action, observation = step
                    steps.append({
                        "tool": action.tool,
                        "input": action.tool_input,
                        "output": observation
                    })
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ SPC å·¥å…·è¢«èª¿ç”¨ä¸”è¼¸å‡ºè¢«æˆªæ–·
            for step in steps:
                if step["tool"] == "spc_query":
                    full_output = step["output"]
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å››å€‹å» åˆ¥çš„ç¯„ä¾‹
                    if "TFT6" in full_output and ("CF6" not in full_output or "LCD6" not in full_output or "USL" not in full_output):
                        print("âš ï¸ æª¢æ¸¬åˆ° SPC å·¥å…·è¼¸å‡ºè¢«æˆªæ–·ï¼Œä½¿ç”¨å®Œæ•´è¼¸å‡º")
                        complete_output = self.tool_manager.execute_tool("spc_query", query)
                        current_model = llm_model or self.llm.model_name
                        model_prefix = f"**{current_model.upper()}**: "
                        return {
                            "answer": model_prefix + complete_output,
                            "confidence": 0.9,
                            "source": "langchain_agent_fixed",
                            "execution_steps": steps,
                            "total_steps": len(steps),
                            "model_used": current_model
                        }
                    # å¦‚æœå·¥å…·è¼¸å‡ºå®Œæ•´ï¼Œä½† LLM å›ç­”ç°¡åŒ–äº†ï¼Œç›´æ¥è¿”å›å·¥å…·è¼¸å‡º
                    elif "TFT6" in full_output and "CF6" in full_output and "LCD6" in full_output and "USL" in full_output:
                        if len(result["output"]) < len(full_output) * 0.7:  # å¦‚æœ LLM å›ç­”æ˜é¡¯æ¯”å·¥å…·è¼¸å‡ºçŸ­
                            print("âš ï¸ æª¢æ¸¬åˆ° LLM ç°¡åŒ–äº† SPC å·¥å…·è¼¸å‡ºï¼Œè¿”å›å®Œæ•´å·¥å…·å›æ‡‰")
                            current_model = llm_model or self.llm.model_name
                            model_prefix = f"**{current_model.upper()}**: "
                            return {
                                "answer": model_prefix + full_output,
                                "confidence": 0.9,
                                "source": "langchain_agent_full_output",
                                "execution_steps": steps,
                                "total_steps": len(steps),
                                "model_used": current_model
                            }
            
            current_model = self.llm.model_name
            
            # æ§‹å»ºå¸¶æ¨¡å‹åç¨±çš„å›ç­”
            model_prefix = f"**{current_model.upper()}**: "
            final_answer = model_prefix + result["output"]
            
            # æ¢å¾©åŸå§‹æ¨¡å‹ï¼ˆå¦‚æœæœ‰æ”¹è®Šï¼‰
            if llm_model and llm_model != original_model:
                print(f"ğŸ”„ æ¢å¾©æ¨¡å‹: {llm_model} â†’ {original_model}")
                self.llm = original_llm
                self.agent_executor = self._create_agent()
            
            return {
                "answer": final_answer,
                "confidence": 0.8,
                "source": "langchain_agent",
                "execution_steps": steps,
                "total_steps": len(steps),
                "model_used": current_model
            }
            
        except Exception as e:
            error_str = str(e)
            print(f"âŒ LangChain Agent åŸ·è¡ŒéŒ¯èª¤: {error_str}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º API ç›¸é—œéŒ¯èª¤ï¼Œæä¾›æ›´æ˜ç¢ºçš„éŒ¯èª¤è¨Šæ¯
            if "429" in error_str or "è¶…éä½¿ç”¨è€…æ¯æ—¥æœ€å¤§ä½¿ç”¨é‡" in error_str:
                error_message = "ğŸš« API ä½¿ç”¨é‡å·²é”æ¯æ—¥é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–åˆ‡æ›å…¶ä»–æ¨¡å‹ã€‚"
            elif "401" in error_str or "Unauthorized" in error_str:
                error_message = "ğŸ”‘ API é‡‘é‘°èªè­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®šã€‚"
            elif "403" in error_str or "Forbidden" in error_str:
                error_message = "â›” API æ¬Šé™ä¸è¶³ï¼Œè«‹æª¢æŸ¥ API é‡‘é‘°æ¬Šé™ã€‚"
            elif "timeout" in error_str.lower() or "è¶…æ™‚" in error_str:
                error_message = "â° API è«‹æ±‚è¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šæˆ–ç¨å¾Œå†è©¦ã€‚"
            elif "connection" in error_str.lower() or "é€£ç·š" in error_str:
                error_message = "ğŸŒ ç¶²è·¯é€£ç·šå•é¡Œï¼Œè«‹æª¢æŸ¥ç¶²è·¯ç‹€æ…‹ã€‚"
            else:
                # é™ç´šåˆ°ç°¡å–®å›æ‡‰
                fallback_response = self._fallback_response(query)
                return {
                    "answer": fallback_response,
                    "confidence": 0.3,
                    "source": "langchain_agent_fallback",
                    "execution_steps": [],
                    "error": error_str
                }
            
            return {
                "answer": error_message,
                "confidence": 0.0,
                "source": "langchain_agent_error",
                "execution_steps": [],
                "error": error_str
            }
    
    def _fallback_response(self, query: str) -> str:
        """ç•¶ LangChain Agent å¤±æ•—æ™‚çš„é™ç´šå›æ‡‰"""
        # ä½¿ç”¨å·¥å…·ç®¡ç†å™¨åŸ·è¡Œå·¥å…·
        if any(keyword in query for keyword in ["è¨ˆç®—", "åŠ ", "æ¸›", "ä¹˜", "é™¤", "+", "-", "*", "/"]):
            return self.tool_manager.execute_tool("calculation", query)
        elif any(keyword in query for keyword in ["æ™‚é–“", "å¹¾é»", "ç¾åœ¨"]):
            return self.tool_manager.execute_tool("current_time", query)
        elif any(keyword in query for keyword in ["SPC"]):
            return self.tool_manager.execute_tool("spc_query", query)
        elif any(keyword in query for keyword in ["EDC"]) and any(keyword in query for keyword in ["æ ¼å¼", "XML"]):
            return self.tool_manager.execute_tool("edc_format_check", query)
        elif any(keyword in query for keyword in ["EDC"]):
            return self.tool_manager.execute_tool("edc_query", query)
        elif any(keyword in query for keyword in ["IP", "è¨­å®š", "æ©Ÿå°"]):
            return self.tool_manager.execute_tool("ip_edc_config_check", query)
        else:
            return f"âš ï¸ æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†é€™å€‹å•é¡Œï¼šã€Œ{query}ã€\n\nå¯ä»¥å˜—è©¦ï¼š\nâ€¢ ä½¿ç”¨æ›´å…·é«”çš„æè¿°\nâ€¢ æª¢æŸ¥æ˜¯å¦æ¶‰åŠè¨ˆç®—ã€æ™‚é–“ã€SPCã€EDC æˆ– IP è¨­å®šç›¸é—œå•é¡Œ\nâ€¢ æˆ–åˆ‡æ›å…¶ä»–å¯ç”¨çš„æ¨¡å‹"
    
    def get_tool_info(self) -> Dict[str, Any]:
        """ç²å–å·¥å…·è³‡è¨Š"""
        return self.tool_manager.get_tool_info()
    
    def list_available_tools(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨å·¥å…·"""
        return self.tool_manager.list_tools()
