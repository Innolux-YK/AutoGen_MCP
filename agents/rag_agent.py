"""
RAG ä»£ç† - è² è²¬çŸ¥è­˜åº«æª¢ç´¢å’Œå›ç­”ç”Ÿæˆ
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
import config
from embedding_service import get_embedding_service
from services.llm_service import LLMService

class RAGAgent:
    """RAG ä»£ç† - çŸ¥è­˜åº«æª¢ç´¢å’Œå›ç­”ç”Ÿæˆ"""
    
    def __init__(self):
        try:
            # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
            print("ğŸ“š åˆå§‹åŒ– RAG ä»£ç†...")
            self.chroma_client = chromadb.PersistentClient(
                path=config.VECTOR_DB_PATH,
                settings=Settings(anonymized_telemetry=False)
            )
            
            # ç²å– collection
            try:
                self.collection = self.chroma_client.get_collection("documents")
                print(f"âœ… é€£æ¥åˆ°ç¾æœ‰ collectionï¼Œæ–‡æª”æ•¸é‡: {self.collection.count()}")
            except:
                print("âš ï¸  æœªæ‰¾åˆ°ç¾æœ‰ collectionï¼Œè«‹å…ˆåŸ·è¡Œ document_processor.py")
                self.collection = None
            
            # åˆå§‹åŒ– embedding æœå‹™
            self.embedding_service = get_embedding_service()
            
            # åˆå§‹åŒ– LLM æœå‹™
            self.llm_service = LLMService()
            
            print("âœ… RAG ä»£ç†åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ RAG ä»£ç†åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def search_and_answer(self, query: str, chat_history: List[Dict] = None, llm_model: str = None) -> Dict[str, Any]:
        """
        æœå°‹çŸ¥è­˜åº«ä¸¦ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            chat_history: å°è©±æ­·å²
            llm_model: æŒ‡å®šä½¿ç”¨çš„LLMæ¨¡å‹
            
        Returns:
            åŒ…å«å›ç­”å’Œç›¸é—œæ–‡æª”çš„å­—å…¸
        """
        if not self.collection:
            return {
                "answer": "æŠ±æ­‰ï¼ŒçŸ¥è­˜åº«å°šæœªå»ºç«‹ã€‚è«‹å…ˆåŸ·è¡Œæ–‡æª”è™•ç†ç¨‹åºã€‚",
                "source_documents": [],
                "confidence": 0.0
            }
        
        try:
            # 1. å‘é‡æª¢ç´¢
            print(f"ğŸ” æœå°‹ç›¸é—œæ–‡æª”: {query[:50]}...")
            search_results = self._search_documents(query, n_results=5)
            
            if not search_results["documents"] or not search_results["documents"][0]:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è­˜åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚è«‹å˜—è©¦å…¶ä»–å•é¡Œæˆ–ç¢ºèªå•é¡Œè¡¨é”ã€‚",
                    "source_documents": [],
                    "confidence": 0.0
                }
            
            # 2. çµ„ç¹”æª¢ç´¢çµæœä¸¦åŠ å…¥ç›¸ä¼¼åº¦éæ¿¾
            relevant_docs = []
            for i, (doc, metadata, distance) in enumerate(zip(
                search_results["documents"][0],
                search_results["metadatas"][0],
                search_results["distances"][0] if "distances" in search_results else [0] * len(search_results["documents"][0])
            )):
                # æª¢æŸ¥ç›¸ä¼¼åº¦é–€æª»
                if distance > config.SIMILARITY_THRESHOLD:
                    print(f"ğŸ“„ éæ¿¾æ–‡æª”ï¼šè·é›¢ {distance:.3f} > é–€æª» {config.SIMILARITY_THRESHOLD}")
                    continue
                    
                relevant_docs.append({
                    "content": doc,
                    "metadata": metadata,
                    "distance": distance,
                    "source_file": metadata.get("source_file", "Unknown"),
                    "title": metadata.get("title", ""),
                    "chunk_index": metadata.get("chunk_index", 0),
                    "images": metadata.get("images", "").split("|") if metadata.get("images") else []
                })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆè·é›¢è¶Šå°è¶Šå‰é¢ï¼‰
            relevant_docs.sort(key=lambda x: x.get("distance", float('inf')))
            print(f"ğŸ“„ éæ¿¾å¾Œæ–‡æª”æ•¸é‡ï¼š{len(relevant_docs)}ï¼ˆé–€æª»: {config.SIMILARITY_THRESHOLD}ï¼‰")
            
            # 3. ç”Ÿæˆå›ç­”
            answer = self._generate_answer(query, relevant_docs, chat_history, llm_model)
            
            # 4. è¨ˆç®—ä¿¡å¿ƒåº¦
            confidence = self._calculate_confidence(search_results, relevant_docs)
            
            return {
                "answer": answer,
                "source_documents": relevant_docs,
                "confidence": confidence
            }
            
        except Exception as e:
            error_str = str(e)
            print(f"âŒ RAG æœå°‹éŒ¯èª¤: {error_str}")
            
            # ç›´æ¥é¡¯ç¤ºåŸå§‹éŒ¯èª¤åŸå› ï¼Œè®“ç”¨æˆ¶äº†è§£å…·é«”å•é¡Œ
            return {
                "answer": f"âŒ RAG ç³»çµ±éŒ¯èª¤ï¼š{error_str}",
                "source_documents": [],
                "confidence": 0.0
            }
    
    def _search_documents(self, query: str, n_results: int = 5) -> Dict[str, Any]:
        """æœå°‹ç›¸é—œæ–‡æª”"""
        try:
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self.embedding_service.encode([query])
            
            # åŸ·è¡Œå‘é‡æœå°‹
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            
            return results
            
        except Exception as e:
            error_str = str(e)
            print(f"æ–‡æª”æœå°‹éŒ¯èª¤: {error_str}")
            
            # ç›´æ¥å‘ä¸Šæ‹‹å‡ºéŒ¯èª¤ï¼Œè®“ä¸Šå±¤è™•ç†
            raise
    
    def _generate_answer(self, query: str, relevant_docs: List[Dict], chat_history: List[Dict] = None, llm_model: str = None) -> str:
        """ä½¿ç”¨æª¢ç´¢åˆ°çš„æ–‡æª”ç”Ÿæˆå›ç­”"""
        try:
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = self._build_context(relevant_docs)
            
            # æ§‹å»ºæç¤ºè©
            prompt = self._build_prompt(query, context, chat_history)
            
            # ç”Ÿæˆå›ç­”ï¼ˆä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ï¼‰
            answer = self.llm_service.generate_response(prompt, model=llm_model)
            
            # æ·»åŠ æ¨¡å‹åç¨±å‰ç¶´
            used_model = llm_model or config.INNOAI_DEFAULT_MODEL
            model_prefix = f"**{used_model.upper()}**: "
            final_answer = model_prefix + answer
            
            return final_answer
            
        except Exception as e:
            print(f"å›ç­”ç”ŸæˆéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    def _build_context(self, relevant_docs: List[Dict]) -> str:
        """æ§‹å»ºä¸Šä¸‹æ–‡è³‡è¨Š"""
        context_parts = []
        
        for i, doc in enumerate(relevant_docs[:3]):  # åªä½¿ç”¨å‰3å€‹æœ€ç›¸é—œçš„æ–‡æª”
            source = doc.get("source_file", "Unknown")
            title = doc.get("title", "")
            content = doc.get("content", "")
            
            context_part = f"æ–‡æª” {i+1}:\n"
            context_part += f"ä¾†æº: {source}\n"
            if title:
                context_part += f"æ¨™é¡Œ: {title}\n"
            context_part += f"å…§å®¹: {content}\n"
            
            context_parts.append(context_part)
        
        return "\n" + "="*50 + "\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str, chat_history: List[Dict] = None) -> str:
        """æ§‹å»º LLM æç¤ºè©"""
        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æŠ€è¡“æ–‡æª”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡æª”å…§å®¹å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

è¦å‰‡ï¼š
1. åªä½¿ç”¨æä¾›çš„æ–‡æª”å…§å®¹ä¾†å›ç­”å•é¡Œ
2. å¦‚æœæ–‡æª”ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜
3. å›ç­”è¦æº–ç¢ºã€ç°¡æ½”ä¸”æœ‰å¹«åŠ©
4. å¦‚æœæœ‰å¤šå€‹ç›¸é—œè³‡è¨Šï¼Œè«‹æ•´ç†å¾Œæä¾›
5. å¯ä»¥å¼•ç”¨å…·é«”çš„æ–‡æª”ä¾†æº

ç›¸é—œæ–‡æª”å…§å®¹ï¼š
{context}

ç”¨æˆ¶å•é¡Œï¼š{query}

è«‹æä¾›è©³ç´°ä¸”å‡†ç¢ºçš„å›ç­”ï¼š"""

        # å¦‚æœæœ‰å°è©±æ­·å²ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡
        if chat_history and len(chat_history) > 0:
            history_context = "\nå°è©±æ­·å²ï¼š\n"
            for msg in chat_history[-3:]:  # åªä½¿ç”¨æœ€è¿‘3æ¢å°è©±
                role = "ç”¨æˆ¶" if msg["role"] == "user" else "åŠ©æ‰‹"
                history_context += f"{role}: {msg['content']}\n"
            
            prompt = prompt.replace("ç”¨æˆ¶å•é¡Œï¼š", history_context + "\nç”¨æˆ¶å•é¡Œï¼š")
        
        return prompt
    
    def _calculate_confidence(self, search_results: Dict, relevant_docs: List[Dict]) -> float:
        """è¨ˆç®—å›ç­”çš„ä¿¡å¿ƒåº¦"""
        try:
            if not search_results.get("distances") or not search_results["distances"][0]:
                return 0.5  # é»˜èªä¿¡å¿ƒåº¦
            
            # åŸºæ–¼ç›¸ä¼¼åº¦è·é›¢è¨ˆç®—ä¿¡å¿ƒåº¦
            distances = search_results["distances"][0]
            
            if not distances:
                return 0.5
            
            # è·é›¢è¶Šå°ï¼Œä¿¡å¿ƒåº¦è¶Šé«˜
            avg_distance = sum(distances) / len(distances)
            confidence = max(0.0, min(1.0, 1.0 - avg_distance))
            
            return confidence
            
        except Exception as e:
            print(f"ä¿¡å¿ƒåº¦è¨ˆç®—éŒ¯èª¤: {e}")
            return 0.5
    
    def generate_response(self, query: str) -> str:
        """ç”Ÿæˆä¸€èˆ¬å›æ‡‰ï¼ˆä¸ä½¿ç”¨ RAGï¼‰"""
        try:
            prompt = f"""ä½ æ˜¯ä¸€å€‹å‹å–„çš„AIåŠ©æ‰‹ã€‚è«‹å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š

{query}

è«‹æä¾›æœ‰å¹«åŠ©çš„å›ç­”ï¼š"""
            
            return self.llm_service.generate_response(prompt)
            
        except Exception as e:
            print(f"ä¸€èˆ¬å›æ‡‰ç”ŸæˆéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•è™•ç†æ‚¨çš„å•é¡Œã€‚"
    
    def get_database_status(self) -> Dict[str, Any]:
        """ç²å–è³‡æ–™åº«ç‹€æ…‹"""
        try:
            if not self.collection:
                return {"status": "æœªé€£æ¥", "count": 0}
            
            count = self.collection.count()
            return {
                "status": "æ­£å¸¸",
                "count": count,
                "collection_name": "documents"
            }
        except Exception as e:
            return {"status": "éŒ¯èª¤", "error": str(e)}
    
    def get_model_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ç‹€æ…‹"""
        try:
            embedding_info = self.embedding_service.get_model_info()
            llm_info = self.llm_service.get_model_info()
            
            return {
                "embedding_model": embedding_info,
                "llm_model": llm_info,
                "status": "æ­£å¸¸"
            }
        except Exception as e:
            return {"status": "éŒ¯èª¤", "error": str(e)}
