"""
æ–‡æª”è™•ç†å™¨ (Document Processor)

é‡è¦èªªæ˜ï¼š
æ­¤æª”æ¡ˆçš„åŠŸèƒ½æ˜¯ RAG ç³»çµ±çš„è³‡æ–™æº–å‚™éšæ®µï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡æª”è§£æï¼ˆå¾ DOCX æå–æ–‡å­—å’Œåœ–ç‰‡ï¼‰
2. æ–‡å­—é è™•ç†ï¼ˆæ¸…ç†ã€åˆ†å¡Šï¼‰
3. å‘é‡åŒ–ï¼ˆä½¿ç”¨é è¨“ç·´çš„ embedding æ¨¡å‹ï¼‰
4. ç´¢å¼•å»ºç«‹ï¼ˆå­˜å„²åˆ°å‘é‡è³‡æ–™åº«ï¼‰

é€™ **ä¸æ˜¯** çœŸæ­£çš„æ¨¡å‹è¨“ç·´ï¼
- æ²’æœ‰æ¨¡å‹åƒæ•¸æ›´æ–°
- æ²’æœ‰å­¸ç¿’éç¨‹
- åªæ˜¯ä½¿ç”¨ç¾æˆçš„ embedding æ¨¡å‹é€²è¡Œå‘é‡åŒ–

å¦‚æœéœ€è¦çœŸæ­£çš„è¨“ç·´åŠŸèƒ½ï¼Œæ‡‰è©²è€ƒæ…®ï¼š
- Fine-tuning embedding æ¨¡å‹
- è¨“ç·´è‡ªå®šç¾©æª¢ç´¢æ’åºæ¨¡å‹
- åŸºæ–¼ä½¿ç”¨è€…å›é¥‹çš„å¼·åŒ–å­¸ç¿’
"""

import os
import json
import glob
from typing import List, Dict
import chromadb
from chromadb.config import Settings
import config
from utils import (
    extract_text_from_document,  # æ”¯æ´ DOC å’Œ DOCX 
    extract_images_from_document,  # æ”¯æ´ DOC å’Œ DOCX åœ–ç‰‡æå–
    chunk_text, 
    clean_text, 
    save_metadata,
    cleanup_temp_files,  # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    auto_convert_doc_to_docx  # DOC è½‰ DOCX
)
from embedding_service import get_embedding_service

class DocumentProcessor:
    """
    æ–‡æª”è™•ç†å™¨ - è² è²¬æ–‡æª”è§£æã€å‘é‡åŒ–å’Œç´¢å¼•å»ºç«‹
    æ³¨æ„ï¼šé€™ä¸æ˜¯æ¨¡å‹è¨“ç·´ï¼Œè€Œæ˜¯ RAG ç³»çµ±çš„è³‡æ–™æº–å‚™éšæ®µ
    """
    def __init__(self):
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        config.ensure_directories()
        
        # åˆå§‹åŒ–embeddingæœå‹™ï¼ˆä½¿ç”¨é è¨“ç·´æ¨¡å‹ï¼Œéè¨“ç·´ï¼‰
        print("åˆå§‹åŒ–embeddingæœå‹™...")
        self.embedding_service = get_embedding_service()
        print(f"ä½¿ç”¨é è¨“ç·´embeddingæ¨¡å‹: {self.embedding_service.get_model_info()}")
        
        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        print("åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
        self.chroma_client = chromadb.PersistentClient(
            path=config.VECTOR_DB_PATH,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # å‰µå»ºæˆ–ç²å–collection
        try:
            self.collection = self.chroma_client.get_collection("documents")
            print("ä½¿ç”¨ç¾æœ‰çš„collection")
        except:
            self.collection = self.chroma_client.create_collection(
                name="documents",
                metadata={"hnsw:space": "cosine"}
            )
            print("å‰µå»ºæ–°çš„collection")
        
        self.documents_metadata = []
    
    def get_existing_files(self):
        """
        ç²å–å·²è¨“ç·´çš„æª”æ¡ˆåˆ—è¡¨
        """
        try:
            # æŸ¥è©¢æ‰€æœ‰æ–‡æª”ä»¥ç²å–å·²è™•ç†çš„æª”æ¡ˆ
            all_results = self.collection.get()
            if all_results and all_results['metadatas']:
                existing_files = set()
                for metadata in all_results['metadatas']:
                    if 'relative_path' in metadata:
                        existing_files.add(metadata['relative_path'])
                    elif 'source_file' in metadata:
                        existing_files.add(metadata['source_file'])
                return existing_files
        except Exception as e:
            print(f"ç²å–å·²å­˜åœ¨æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return set()
    
    def convert_all_docs_to_docx(self, dataset_path: str = config.DATASET_PATH) -> None:
        """
        å°‡ Dataset ç›®éŒ„ä¸‹æ‰€æœ‰ DOC æª”æ¡ˆè½‰æ›ç‚º DOCX
        é¿å…é‡è¤‡è™•ç†ç›¸åŒæ–‡æª”çš„ä¸åŒæ ¼å¼
        """
        print("=== æª¢æŸ¥ä¸¦è½‰æ› DOC æª”æ¡ˆ ===")
        
        # æœå°‹æ‰€æœ‰ DOC æª”æ¡ˆ
        doc_pattern = os.path.join(dataset_path, "*.doc")
        doc_files = glob.glob(doc_pattern)
        
        if not doc_files:
            print("âœ… æ²’æœ‰æ‰¾åˆ°éœ€è¦è½‰æ›çš„ DOC æª”æ¡ˆ")
            return
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(doc_files)} å€‹ DOC æª”æ¡ˆï¼Œæº–å‚™è½‰æ›ç‚º DOCX")
        
        converted_count = 0
        
        for doc_file in doc_files:
            filename = os.path.basename(doc_file)
            print(f"ğŸ”„ è½‰æ›: {filename}")
            
            try:
                # è½‰æ› DOC åˆ° DOCXï¼ˆæ°¸ä¹…è½‰æ›ï¼‰
                docx_file = auto_convert_doc_to_docx(doc_file, permanent=True)
                
                if docx_file and os.path.exists(docx_file):
                    print(f"âœ… è½‰æ›æˆåŠŸ: {os.path.basename(docx_file)}")
                    
                    # åˆªé™¤åŸ DOC æª”æ¡ˆ
                    try:
                        os.remove(doc_file)
                        print(f"ğŸ—‘ï¸  å·²åˆªé™¤åŸæª”æ¡ˆ: {filename}")
                        converted_count += 1
                    except Exception as e:
                        print(f"âš ï¸  ç„¡æ³•åˆªé™¤åŸæª”æ¡ˆ {filename}: {e}")
                        converted_count += 1  # è½‰æ›é‚„æ˜¯æˆåŠŸäº†
                else:
                    print(f"âŒ è½‰æ›å¤±æ•—: {filename}")
                    
            except Exception as e:
                print(f"âŒ è½‰æ› {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        if converted_count > 0:
            print(f"âœ… æˆåŠŸè½‰æ›ä¸¦åˆªé™¤äº† {converted_count} å€‹ DOC æª”æ¡ˆ")
        print()

    def process_docx_files(self, dataset_path: str = config.DATASET_PATH, force_reprocess: bool = False):
        """
        è™•ç†æŒ‡å®šç›®éŒ„ä¸‹çš„æ‰€æœ‰DOCXæª”æ¡ˆï¼ˆæ³¨æ„ï¼šé€™æ˜¯æ–‡æª”ç´¢å¼•å»ºç«‹ï¼Œä¸æ˜¯æ¨¡å‹è¨“ç·´ï¼‰
        
        Args:
            dataset_path: æ•¸æ“šé›†è·¯å¾‘
            force_reprocess: æ˜¯å¦å¼·åˆ¶é‡æ–°è™•ç†æ‰€æœ‰æª”æ¡ˆ
        """
        # é¦–å…ˆè½‰æ›æ‰€æœ‰ DOC æª”æ¡ˆç‚º DOCX
        self.convert_all_docs_to_docx(dataset_path)
        
        print("=== æ–‡æª”å¢é‡è™•ç†é–‹å§‹ ===")
        
        dataset_path = config.DATASET_PATH
        if not os.path.exists(dataset_path):
            print(f"Datasetç›®éŒ„ä¸å­˜åœ¨: {dataset_path}")
            return
        
        # ç²å–å·²è™•ç†çš„æª”æ¡ˆåˆ—è¡¨
        existing_files = set() if force_reprocess else self.get_existing_files()
        print(f"å·²è™•ç†çš„æª”æ¡ˆæ•¸é‡: {len(existing_files)}")
        
        # éè¿´æœå°‹æ‰€æœ‰DOCXæª”æ¡ˆï¼ˆDOCå·²ç¶“è½‰æ›ç‚ºDOCXï¼‰
        docx_files = []
        new_files = []
        
        for root, dirs, files in os.walk(dataset_path):
            for file in files:
                # åªè™•ç† .docx æª”æ¡ˆï¼ˆDOC å·²è½‰æ›ï¼‰
                if file.endswith('.docx') and not file.startswith('~$'):  # æ’é™¤è‡¨æ™‚æª”æ¡ˆ
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, dataset_path)
                    docx_files.append((file_path, relative_path))
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºæ–°æª”æ¡ˆ
                    if relative_path not in existing_files:
                        new_files.append((file_path, relative_path))
        
        print(f"ç¸½å…±æ‰¾åˆ° {len(docx_files)} å€‹ DOCX æª”æ¡ˆ")
        print(f"éœ€è¦è™•ç†çš„æ–°æª”æ¡ˆ: {len(new_files)} å€‹")
        
        if not new_files:
            if not docx_files:
                print("Datasetç›®éŒ„åŠå­ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ° DOCX æª”æ¡ˆ")
            else:
                print("æ‰€æœ‰æª”æ¡ˆéƒ½å·²ç¶“è™•ç†éäº†ï¼Œç„¡éœ€é‡æ–°è™•ç†")
            return
        
        print("æ–°æª”æ¡ˆåˆ—è¡¨:")
        for file_path, relative_path in new_files:
            print(f"  - {relative_path}")
        print()
        
        all_texts = []
        all_metadatas = []
        all_ids = []
        
        # ç²å–ç¾æœ‰æ–‡æª”çš„æœ€å¤§ç´¢å¼•ï¼Œé¿å…IDè¡çª
        existing_doc_count = len(existing_files)
        
        for i, (file_path, relative_path) in enumerate(new_files):
            filename = os.path.basename(file_path)
            print(f"è™•ç†æ–°æª”æ¡ˆ ({i+1}/{len(new_files)}): {relative_path}")
            
            try:
                # æå–æ–‡å­— (æ”¯æ´ DOC å’Œ DOCX)
                text_data = extract_text_from_document(file_path)
                
                if not text_data['full_text']:
                    print(f"è­¦å‘Š: {relative_path} æ²’æœ‰æå–åˆ°æ–‡å­—å…§å®¹")
                    continue
                
                # æå–åœ–ç‰‡ (æ”¯æ´ DOCX å’Œ DOC æª”æ¡ˆ)
                image_paths = extract_images_from_document(file_path, config.IMAGES_PATH)
                print(f"å¾ {relative_path} æå–äº† {len(image_paths)} å¼µåœ–ç‰‡")
                
                # æ¸…ç†ä¸¦åˆ†å‰²æ–‡å­—
                cleaned_text = clean_text(text_data['full_text'])
                text_chunks = chunk_text(cleaned_text, config.CHUNK_SIZE, config.CHUNK_OVERLAP)
                
                # ç‚ºæ¯å€‹æ–‡å­—å¡Šå‰µå»ºè¨˜éŒ„ï¼Œä½¿ç”¨æ–°çš„ç´¢å¼•é¿å…è¡çª
                doc_index = existing_doc_count + i
                for j, chunk in enumerate(text_chunks):
                    chunk_id = f"doc_{doc_index}_{j}"
                    
                    # ChromaDB metadataåªèƒ½å­˜å„²åŸºæœ¬é¡å‹ï¼Œå°‡listè½‰æ›ç‚ºå­—ç¬¦ä¸²
                    metadata = {
                        "source_file": filename,
                        "relative_path": relative_path,  # æ·»åŠ ç›¸å°è·¯å¾‘è³‡è¨Š
                        "title": text_data['title'],
                        "keywords": "|".join(text_data['keywords']) if text_data['keywords'] else "",
                        "chunk_index": j,
                        "total_chunks": len(text_chunks),
                        "images": "|".join(image_paths) if image_paths else "",
                        "file_path": file_path
                    }
                    
                    all_texts.append(chunk)
                    all_metadatas.append(metadata)
                    all_ids.append(chunk_id)
                    
                    # ä¿å­˜æ–‡æª”å…ƒæ•¸æ“š
                    self.documents_metadata.append({
                        "id": chunk_id,
                        "title": text_data['title'],
                        "keywords": text_data['keywords'],
                        "content": chunk,
                        "source_file": filename,
                        "relative_path": relative_path,
                        "images": image_paths,
                        "file_path": file_path
                    })
                    
            except Exception as e:
                print(f"è™•ç†æª”æ¡ˆ {relative_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        if all_texts:
            print(f"é–‹å§‹å‘é‡åŒ– {len(all_texts)} å€‹æ–°æ–‡å­—å¡Š...")
            
            # ç”Ÿæˆembeddingsï¼ˆä½¿ç”¨é è¨“ç·´æ¨¡å‹ï¼‰
            embeddings = self.embedding_service.encode(all_texts)
            
            # å­˜å„²åˆ°å‘é‡è³‡æ–™åº«
            self.collection.add(
                documents=all_texts,
                metadatas=all_metadatas,
                ids=all_ids,
                embeddings=embeddings  # å·²ç¶“æ˜¯listæ ¼å¼ï¼Œä¸éœ€è¦tolist()
            )
            
            print(f"æˆåŠŸè™•ç†ä¸¦å­˜å„²äº† {len(all_texts)} å€‹æ–°æ–‡å­—å¡Š")
            
            # æ›´æ–°æ–‡æª”å…ƒæ•¸æ“šï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
            metadata_file = os.path.join(config.MODEL_PATH, "documents_metadata.json")
            
            # è®€å–ç¾æœ‰å…ƒæ•¸æ“š
            existing_metadata = []
            if os.path.exists(metadata_file):
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        existing_metadata = json.load(f)
                except:
                    existing_metadata = []
            
            # åˆä½µæ–°èˆŠå…ƒæ•¸æ“š
            all_metadata = existing_metadata + self.documents_metadata
            save_metadata(all_metadata, metadata_file)
            print(f"å…ƒæ•¸æ“šå·²æ›´æ–°è‡³: {metadata_file}")
            
            # æ¸…ç†è‡¨æ™‚è½‰æ›æª”æ¡ˆ
            print("\nğŸ—‘ï¸  æ¸…ç†è‡¨æ™‚æª”æ¡ˆ...")
            cleanup_temp_files(config.DATASET_PATH)
            
        else:
            print("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–°æ–‡å­—å…§å®¹é€²è¡Œè™•ç†")
    
    def search_similar_documents(self, query: str, n_results: int = 5):
        """
        æœå°‹ç›¸ä¼¼æ–‡æª”
        """
        query_embedding = self.embedding_service.encode([query])
        
        results = self.collection.query(
            query_embeddings=query_embedding,  # å·²ç¶“æ˜¯listæ ¼å¼
            n_results=n_results
        )
        
        return results
    
    def get_collection_info(self):
        """
        ç²å–collectionè³‡è¨Š
        """
        count = self.collection.count()
        print(f"å‘é‡è³‡æ–™åº«ä¸­å…±æœ‰ {count} å€‹æ–‡æª”")
        return count

def main():
    """
    ä¸»å‡½æ•¸ - æ–‡æª”è™•ç†å’Œå‘é‡åŒ–ç´¢å¼•å»ºç«‹
    æ³¨æ„ï¼šé€™ä¸æ˜¯æ¨¡å‹è¨“ç·´ï¼Œè€Œæ˜¯ RAG ç³»çµ±çš„è³‡æ–™æº–å‚™éç¨‹
    """
    import sys
    
    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸
    force_reprocess_mode = False
    incremental_mode = False
    
    if len(sys.argv) > 1:
        if sys.argv[1] == '--force-retrain':  # ä¿æŒåƒæ•¸åç¨±å‘å¾Œå…¼å®¹
            force_reprocess_mode = True
            print("=== æ–‡æª”é‡æ–°è™•ç†é–‹å§‹ ===")
        elif sys.argv[1] == '--incremental':
            incremental_mode = True
            print("=== æ–‡æª”å¢é‡è™•ç†é–‹å§‹ ===")
        else:
            print("=== æ–‡æª”è™•ç†é–‹å§‹ ===")
    else:
        print("=== æ–‡æª”è™•ç†å’Œå‘é‡åŒ–ç´¢å¼•å»ºç«‹ ===")
    
    processor = DocumentProcessor()
    
    # æª¢æŸ¥ç¾æœ‰è³‡æ–™
    existing_count = processor.get_collection_info()
    
    if force_reprocess_mode:
        # å¼·åˆ¶é‡æ–°è™•ç†æ¨¡å¼ï¼ˆç”±æ‰¹è™•ç†æª”æ¡ˆèª¿ç”¨ï¼‰
        if existing_count > 0:
            print(f"æ¸…ç©ºç¾æœ‰çš„ {existing_count} å€‹æ–‡æª”")
            # æ¸…ç©ºç¾æœ‰è³‡æ–™
            processor.chroma_client.delete_collection("documents")
            processor.collection = processor.chroma_client.create_collection(
                name="documents",
                metadata={"hnsw:space": "cosine"}
            )
            print("å·²æ¸…ç©ºç¾æœ‰è³‡æ–™")
        print("é–‹å§‹å®Œå…¨é‡æ–°è™•ç†...")
        processor.process_docx_files(force_reprocess=True)
    elif incremental_mode:
        # å¢é‡è™•ç†æ¨¡å¼ï¼ˆç”±æ‰¹è™•ç†æª”æ¡ˆèª¿ç”¨ï¼‰
        print(f"å‘é‡è³‡æ–™åº«ä¸­å·²æœ‰ {existing_count} å€‹æ–‡æª”")
        print("é–‹å§‹å¢é‡è™•ç†...")
        processor.process_docx_files(force_reprocess=False)
    elif existing_count > 0:
        print(f"\nå‘é‡è³‡æ–™åº«ä¸­å·²æœ‰ {existing_count} å€‹æ–‡æª”")
        print("é¸æ“‡è™•ç†æ¨¡å¼:")
        print("1. å¢é‡è™•ç† (åªè™•ç†æ–°æª”æ¡ˆï¼Œæ¨è–¦)")
        print("2. å®Œå…¨é‡æ–°è™•ç† (æ¸…ç©ºæ‰€æœ‰è³‡æ–™é‡æ–°é–‹å§‹)")
        print("3. å–æ¶ˆè™•ç†")
        
        while True:
            choice = input("è«‹é¸æ“‡ (1/2/3): ").strip()
            if choice == "1":
                print("=== é–‹å§‹å¢é‡è™•ç† ===")
                processor.process_docx_files(force_reprocess=False)
                break
            elif choice == "2":
                print("=== é–‹å§‹å®Œå…¨é‡æ–°è™•ç† ===")
                # æ¸…ç©ºç¾æœ‰è³‡æ–™
                processor.chroma_client.delete_collection("documents")
                processor.collection = processor.chroma_client.create_collection(
                    name="documents",
                    metadata={"hnsw:space": "cosine"}
                )
                print("å·²æ¸…ç©ºç¾æœ‰è³‡æ–™")
                processor.process_docx_files(force_reprocess=True)
                break
            elif choice == "3":
                print("å–æ¶ˆè™•ç†")
                return
            else:
                print("ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
    else:
        print("å‘é‡è³‡æ–™åº«ç‚ºç©ºï¼Œé–‹å§‹åˆå§‹è™•ç†...")
        processor.process_docx_files(force_reprocess=True)
    
    # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
    final_count = processor.get_collection_info()
    
    print("\n=== æ–‡æª”è™•ç†å®Œæˆ ===")
    print(f"å‘é‡è³‡æ–™åº«ä¸­ç¸½å…±æœ‰ {final_count} å€‹æ–‡å­—å¡Š")
    
    # æ¸¬è©¦æœå°‹åŠŸèƒ½
    print("\n=== æ¸¬è©¦æœå°‹åŠŸèƒ½ ===")
    test_query = "Windows"
    print(f"æ¸¬è©¦æŸ¥è©¢: '{test_query}'")
    
    results = processor.search_similar_documents(test_query, 3)
    
    if results['documents']:
        print("æœå°‹çµæœ:")
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
            print(f"\nçµæœ {i+1}:")
            print(f"æª”æ¡ˆ: {metadata['source_file']}")
            print(f"æ¨™é¡Œ: {metadata['title']}")
            print(f"å…§å®¹é è¦½: {doc[:100]}...")
    else:
        print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œçµæœ")
    
    print("\n" + "="*50)
    print("æ–‡æª”è™•ç†å’Œç´¢å¼•å»ºç«‹å·²å®Œæˆï¼")
    print("ç³»çµ±å·²æº–å‚™å¥½å›ç­”å•é¡Œ")
    print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ qa_app.py ä¾†é€²è¡Œæ™ºèƒ½å•ç­”")
    print("="*50)

if __name__ == "__main__":
    main()
